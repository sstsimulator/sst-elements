@misc{gem5,
      title={The gem5 Simulator: Version 20.0+},
      author={Jason Lowe-Power and Abdul Mutaal Ahmad and Ayaz Akram and Mohammad Alian and Rico Amslinger and Matteo Andreozzi and Adrià Armejach and Nils Asmussen and Brad Beckmann and Srikant Bharadwaj and Gabe Black and Gedare Bloom and Bobby R. Bruce and Daniel Rodrigues Carvalho and Jeronimo Castrillon and Lizhong Chen and Nicolas Derumigny and Stephan Diestelhorst and Wendy Elsasser and Carlos Escuin and Marjan Fariborz and Amin Farmahini-Farahani and Pouya Fotouhi and Ryan Gambord and Jayneel Gandhi and Dibakar Gope and Thomas Grass and Anthony Gutierrez and Bagus Hanindhito and Andreas Hansson and Swapnil Haria and Austin Harris and Timothy Hayes and Adrian Herrera and Matthew Horsnell and Syed Ali Raza Jafri and Radhika Jagtap and Hanhwi Jang and Reiley Jeyapaul and Timothy M. Jones and Matthias Jung and Subash Kannoth and Hamidreza Khaleghzadeh and Yuetsu Kodama and Tushar Krishna and Tommaso Marinelli and Christian Menard and Andrea Mondelli and Miquel Moreto and Tiago Mück and Omar Naji and Krishnendra Nathella and Hoa Nguyen and Nikos Nikoleris and Lena E. Olson and Marc Orr and Binh Pham and Pablo Prieto and Trivikram Reddy and Alec Roelke and Mahyar Samani and Andreas Sandberg and Javier Setoain and Boris Shingarov and Matthew D. Sinclair and Tuan Ta and Rahul Thakur and Giacomo Travaglini and Michael Upton and Nilay Vaish and Ilias Vougioukas and William Wang and Zhengrong Wang and Norbert Wehn and Christian Weis and David A. Wood and Hongil Yoon and Éder F. Zulian},
      year={2020},
      eprint={2007.03152},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2007.03152},
}

@INPROCEEDINGS{astra,
  author={Won, William and Heo, Taekyung and Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar},
  booktitle={2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  title={ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale},
  year={2023},
  volume={},
  number={},
  pages={283-294},
  keywords={Training;Semiconductor device modeling;Analytical models;Network topology;Systems modeling;Throughput;Data models;Distributed training;High-performance training;Multi-dimensional network;Disaggregated memory system},
  doi={10.1109/ISPASS57527.2023.00035}}

@Inbook{ns3,
author="Riley, George F.
and Henderson, Thomas R.",
editor="Wehrle, Klaus
and G{\"u}ne{\c{s}}, Mesut
and Gross, James",
title="The ns-3 Network Simulator",
bookTitle="Modeling and Tools for Network Simulation",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="15--34",
abstract="As networks of computing devices grow larger and more complex, the need for highly accurate and scalable network simulation technologies becomes critical. Despite the emergence of large-scale testbeds for network research, simulation still plays a vital role in terms of scalability (both in size and in experimental speed), reproducibility, rapid prototyping, and education. With simulation based studies, the approach can be studied in detail at varying scales, with varying data applications, varying field conditions, and will result in reproducible and analyzable results.",
isbn="978-3-642-12331-3",
doi="10.1007/978-3-642-12331-3_2",
url="https://doi.org/10.1007/978-3-642-12331-3_2"
}

@inproceedings{heterogarnet,
  author={Bharadwaj, Srikant and Yin, Jieming and Beckmann, Bradford and Krishna, Tushar},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  title={Kite: A Family of Heterogeneous Interposer Topologies Enabled via Accurate Interconnect Modeling},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/DAC18072.2020.9218539}
}

@INPROCEEDINGS{garnet,
  author={Agarwal, Niket and Krishna, Tushar and Peh, Li-Shiuan and Jha, Niraj K.},
  booktitle={2009 IEEE International Symposium on Performance Analysis of Systems and Software},
  title={GARNET: A detailed on-chip network model inside a full-system simulator},
  year={2009},
  volume={},
  number={},
  pages={33-42},
  keywords={Garnets;Network-on-a-chip;Multiprocessor interconnection networks;Power system interconnection;System-on-a-chip;Proposals;Switches;Computational modeling;Microprocessors;Wire},
  doi={10.1109/ISPASS.2009.4919636}}

@article{SimMPI,
title = {Performance Modeling and Evaluation of MPI},
journal = {Journal of Parallel and Distributed Computing},
volume = {61},
number = {2},
pages = {202-223},
year = {2001},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.2000.1677},
url = {https://www.sciencedirect.com/science/article/pii/S0743731500916770},
author = {Khalid Al-Tawil and Csaba Andras Moritz},
keywords = {LogP, MPI, LogGP, parallel processing, workstations},
abstract = {Users of parallel machines need to have a good grasp for how different communication patterns and styles affect the performance of message-passing applications. LogGP is a simple performance model that reflects the most important parameters required to estimate the communication performance of parallel computers. The message passing interface (MPI) standard provides new opportunities for developing high performance parallel and distributed applications. In this paper, we use LogGP as a conceptual framework for evaluating the performance of MPI communications on three platforms: Cray-Research T3D, Convex Exemplar 1600SP, and a network of workstations (NOW). We develop a simple set of communication benchmarks to extract the LogGP parameters. Our objective in this is to compare the performance of MPI communication on several platforms and to identify a performance model suitable for MPI performance characterization. In particular, two problems are addressed: how LogGP quantifies MPI performance and what extra features are required for modeling MPI, and how MPI performance compare on the three computing platforms: Cray Research T3D, Convex Exemplar 1600SP, and workstations clusters.}
}

@INPROCEEDINGS{stonne,
  author =       {Francisco Mu{\~n}oz-Mart{\'i}nez and Jos{\'e} L. Abell{\'a}n and Manuel E. Acacio and Tushar Krishna},
  title =        {STONNE: Enabling Cycle-Level Microarchitectural Simulation for DNN Inference Accelerators},
  booktitle =    {2021 IEEE International Symposium on Workload Characterization (IISWC)},
  year =         {2021},
  volume =       {},
  number =       {},
  pages =        {},
}


@article{codes,
  title={Enabling parallel simulation of large-scale HPC network systems},
  author={Mubarak, Misbah and Carothers, Christopher D and Ross, Robert B and Carns, Philip},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={28},
  number={1},
  pages={87--100},
  year={2016},
  publisher={IEEE}
}

@article{ross,
  title={ROSS: A high-performance, low-memory, modular Time Warp system},
  author={Carothers, Christopher D and Bauer, David and Pearce, Shawn},
  journal={Journal of parallel and distributed computing},
  volume={62},
  number={11},
  pages={1648--1669},
  year={2002},
  publisher={Elsevier}
}

@article{pacsim,
author = {Liu, Changxi and Sabu, Alen and Chaudhari, Akanksha and Kang, Qingxuan and Carlson, Trevor E.},
title = {Pac-Sim: Simulation of Multi-threaded Workloads using Intelligent, Live Sampling},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3680548},
doi = {10.1145/3680548},
abstract = {High-performance, multi-core processors are the key to accelerating workloads in several application domains. To continue to scale performance at the limit of Moore’s Law and Dennard scaling, software and hardware designers have turned to dynamic solutions that adapt to the needs of applications in a transparent, automatic way. For example, modern hardware improves its performance and power efficiency by changing the hardware configuration, like the frequency and voltage of cores, according to a number of parameters, such as the technology used or the workload running at the time. With this level of dynamism, it is essential to simulate next-generation multi-core processors in a way that can both respond to system changes and accurately determine system performance metrics. Currently, no sampled simulation platform can achieve these goals of dynamic, fast, and accurate simulation of multi-threaded workloads.In this work, we propose a solution that allows for fast, accurate simulation in the presence of both hardware and software dynamism. To accomplish this goal, we present Pac-Sim, a novel sampled simulation methodology for fast, accurate sampled simulation that requires no upfront analysis of the workload. With our proposed methodology, it is now possible to simulate long-running dynamically scheduled multi-threaded programs with significant simulation speedups, even in the presence of dynamic hardware events. We evaluate Pac-Sim using the SPEC CPU2017, NPB, and PARSEC multi-threaded benchmarks with both static and dynamic thread scheduling. The experimental results show that Pac-Sim achieves a very low sampling error of 1.63\% and 3.81\% on average for statically and dynamically scheduled benchmarks, respectively. Pac-Sim also demonstrates significant simulation speedups as high as 523.5\texttimes{} (210.3\texttimes{} on average) for the training input set of SPEC CPU2017 running eight threads.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {81},
numpages = {26},
keywords = {Sampled simulation, multi-threaded workloads, dynamic scheduling}
}

@article{sniper,
author = {Liu, Changxi and Sabu, Alen and Chaudhari, Akanksha and Kang, Qingxuan and Carlson, Trevor E.},
title = {Pac-Sim: Simulation of Multi-threaded Workloads using Intelligent, Live Sampling},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3680548},
doi = {10.1145/3680548},
abstract = {High-performance, multi-core processors are the key to accelerating workloads in several application domains. To continue to scale performance at the limit of Moore’s Law and Dennard scaling, software and hardware designers have turned to dynamic solutions that adapt to the needs of applications in a transparent, automatic way. For example, modern hardware improves its performance and power efficiency by changing the hardware configuration, like the frequency and voltage of cores, according to a number of parameters, such as the technology used or the workload running at the time. With this level of dynamism, it is essential to simulate next-generation multi-core processors in a way that can both respond to system changes and accurately determine system performance metrics. Currently, no sampled simulation platform can achieve these goals of dynamic, fast, and accurate simulation of multi-threaded workloads.In this work, we propose a solution that allows for fast, accurate simulation in the presence of both hardware and software dynamism. To accomplish this goal, we present Pac-Sim, a novel sampled simulation methodology for fast, accurate sampled simulation that requires no upfront analysis of the workload. With our proposed methodology, it is now possible to simulate long-running dynamically scheduled multi-threaded programs with significant simulation speedups, even in the presence of dynamic hardware events. We evaluate Pac-Sim using the SPEC CPU2017, NPB, and PARSEC multi-threaded benchmarks with both static and dynamic thread scheduling. The experimental results show that Pac-Sim achieves a very low sampling error of 1.63\% and 3.81\% on average for statically and dynamically scheduled benchmarks, respectively. Pac-Sim also demonstrates significant simulation speedups as high as 523.5\texttimes{} (210.3\texttimes{} on average) for the training input set of SPEC CPU2017 running eight threads.},
journal = {ACM Trans. Archit. Code Optim.},
month = nov,
articleno = {81},
numpages = {26},
keywords = {Sampled simulation, multi-threaded workloads, dynamic scheduling}
}

@misc{rev,
  author = {Leidel, John and Donofrio, David and Taylor, Chris and Kabrick, Ryan and Killough, Lee},
  title = {rev : RISC-V Native CPU Model for SST},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/tactcomplabs/rev}
}

@misc{sst-data,
  author = {Leidel, John and and Taylor, Chris},
  title = {sst-data: An advanced SST Statistics I/O library},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/tactcomplabs/sst-data}
}


@misc{simeng,
  author = {McIntosh-Smith, Simon and Jones, Hal and Price, James and Jones , Jack and Wilkinson, Finn and Muneeb, Rahat and Weaver, Daniel and Cockrean, Alex and Moore, Joseph},
  title = {SimEng},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/UoB-HPC/SimEng/}
}

@misc{ramulator2,
  title={{Ramulator 2.0: A Modern, Modular, and Extensible DRAM Simulator}},
  author={Haocong Luo and Yahya Can Tu\u{g}rul and F. Nisa Bostancı and Ataberk Olgun and A. Giray Ya\u{g}l{\i}k\c{c}{\i} and and Onur Mutlu},
  year={2023},
  archivePrefix={arXiv},
  primaryClass={cs.AR}
}

@INPROCEEDINGS{accelsim,
  author={Khairy, Mahmoud and Shen, Zhesheng and Aamodt, Tor M. and Rogers, Timothy G.},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  title={Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling},
  year={2020},
  volume={},
  number={},
  pages={473-486},
  keywords={GPGPU;Modeling and Simulation},
  doi={10.1109/ISCA45697.2020.00047}}

@misc{apple,
  author = {Norem, Josh},
  title = {Apple Spent $1 Billion on the M3 Tape-Out, Says Analyst},
  year = 2023,
  url = {https://www.extremetech.com/computing/apple-spent-1-billion-on-the-m3-tape-out-says-analyst},
  journal = {ExtremeTech},
  urldate = {2025-06-16}
}

@misc{axios,
  author = {Fried, Ina},
  title = {Up close with the world's largest supercomputer},
  year = 2025,
  url = {https://www.axios.com/2025/01/10/lawrence-livermore-lab-supercomputer},
  journal = {Axios},
  urldate = {2025-06-16}
}

@inproceedings{polarstar,
  title={PolarStar: Expanding the Horizon of Diameter-3 Networks},
  author={Lakhotia, Kartik and Monroe, Laura and Isham, Kelly and Besta, Maciej and Blach, Nils and Hoefler, Torsten and Petrini, Fabrizio},
  booktitle={Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures},
  pages={345--357},
  year={2024}
}

@inproceedings{conservativesync,
  author = {Ayani, Rassul and Rajaei, Hassan},
  year = {1992},
  month = {01},
  pages = {709-717},
  title = {Parallel Simulation Using Conservative Time Windows.},
  doi = {10.1145/167293.167684}
}
