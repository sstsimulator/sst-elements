This describes only the yumyum-specific functionality present in this element.

When using yumyum functionality, the rest of the Scheduler functionality will
remain unaffected.  Yumyum-specific functionality simply augments the
capabilities of the Scheduler to add the ability to simulate fault, error,
and failure modes into SST simulations of systems, as well as a hierarchy of
nodes that allows for cascading faults to affect the ability of the simulated
system to function optimally. This functionality is optional, and will not
affect non-yumyum simulations.

fault model - we follow the model described by Avizienis et al [TSDC 2004]:
 fault - the cause of an error (eg a bug, stuck bit, alpha particle, etc)
 error - the part of state that may lead to failure (eg a bad value in memory)
 failure - incorrect service (eg, premature job termination)
Nodes in the graph may experience a fault activation (resulting in an error),
the error propagates (unless corrected), possibly getting logged as it goes,
until it reaches schedulable nodes, where it may kill the job.

---
node hierarchy
---

To create a node hierarchy, simply create two nodeComponents, joined with a
link with the parent nodeComponent end of the link using the "Child[n]" port,
and the child nodeComponent end of the link using the "Parent[n]" port, where
[n] is a number that iterates from zero to n, where n is the number of links
connected to that node of a single given type.

---
faults
---

Faults are simulated by creating a "faultActivationRate" element under the
params for a nodeComponent.  This will contain CSV specifying the fault
type and corresponding rate:

<faultActivationRate>
  "fault type 1", "1337",
  "fault type 2", "42"
</faultActivationRate>

Where the fault rate (called lambda) occurs at a rate described in
nodeComponent.cc in

void nodeComponent::sendNextFault( std::string faultType )

Fault activations (and the corresponding error messages) are logged by
Scheduler in two files:

- fault.log - ground truth, this describes where and when fault activations
  occur.  Filename is described using the faultLogFileName param.

- error.log - an error log may also be written, as the error resulting
  from the fault activation propagates through the graph.  At each
  component where the error is seen, an error log will be written according to
  the probability given in the errorMessageProbability param:

  <params>
    <errorMessageProbability>
      "fault type 1", "0.5",
      "fault type 2", "0.95"
    </errorMessageProbability>
    ...
  </params>

  Filename is described using the errorLogFilename param

Errors will travel down the hierarchy of nodes, from parent to child
until they reach nodes connected to the schedComponent.  By default, the
nodes will end whatever job they are running by whenever they receive a
fault, or according to some probability that can be defined using the
jobFailureProbability param, in a similar fashion to errorMessageProbability.

  <jobFailureProbability>
    "fault type 1", "0.5"
  </jobFailureProbability>

This jobFailureProbability parameter can be placed on any nodeComponent.  When
an error travels through the node, the jobFailureProbability associated with
that fault type is examined to determine if the job on the scheduled node the
error will eventually arrive at should be killed.  If yes, then the any
scheduled node the error eventually arrives at will kill the job.  If no, then
the error is sent on normally.  This jobFailureProbability check will be done
at every node that has the jobFailureProbability parameter.  Effectively, a job
will be killed if any node between, and including, the initial node which
causes the fault and the schedulable node which receives the error, has a
jobFailureProbability parameter and passes the probability check.

---
error correction
---

When a node receives an error, either because it suffered a fault activation
event, or because a parent sent it an error event, it may be given the
opportunity to correct the error according to a given probability:

  <errorCorrectionProbability>
    "error type 1", "0.5",
    "error type 2", "1"
  </errorCorrectionProbability>

In this example, "error type 1" has a 50/50 chance of being corrected, and
"error type 2" will always be corrected.

If the node corrects the error, the error event will not propagate to any
children of the node, nor will any dependent jobs be killed.  The
jobFailureProbability attribute will not be examined.

Error logs are still be created normally, and the errorMessageProbability will
be examined normally.

---
fault propagation delays
---

The time taken by a fault to travel from node A to node B is by default zero.
This can be adjusted using the errorPropagationDelay parameter on a
nodeComponent:

  <errorPropagationDelay>
    "retransmit", "0", "2",
    "thermal", "1", "1",
    "lost connection", "2", 5"
  </errorPropagationDelay>

The columns in the delay parameter are the fault name, min delay time, max
delay time. If an error type has no configured delay on a node, it will be
transmitted to all children of that node on the same timestep it is
received, instantly propagating through that node.  Otherwise, a
uniformly-distributed random number is selected using the two specified numbers
as bounds.  (inclusive)  The node then waits that length of time before
transmitting the error to its children.

For each error traveling through the graph, the delay is calculated
independently for each link it traverses.

---
yumyum jobs
---

The original Scheduler trace format can still be used to create simulated jobs,
but there is also an optional yumyum format that may be used instead.  This
format uses three columns to describe jobs to be run:

jobID, seconds needed, nodes needed.

Thus a job "12, 1337, 42" indicates that job 12 needs 1337 seconds on 42 nodes.

If this format is to be used, add the following param to schedComponent:

<useYumYumTraceFormat>true</useYumYumTraceFormat>

---
yumyum logging
---

The scheduler can log the jobs run, with the result (finish/fault).  To use
this, add the following to the schedComponent's params:

<jobLogFileName>job.log</jobLogFileName>
<printYumYumJobLog>true</printYumYumJobLog>
<printJobLog>true</printJobLog>

Where job.log is the desired file to print the log.  The log will have the
following format:

[job ID], [start time], [end time], [pass/fault], [nodes used]

If the end time and pass/fault value are -1, then the log entry is for the job
start.  If the pass/fault value is 0, then the log entry is for a job
finishing.  If the pass/fault value is 1, then the error resulting from a
fault activation killed the job, and the job end time is the simulated second
at which it was killed.

"1","0","-1","-1","1.4 1.3 1.2 1.1"             # job start
"1","0","100000","0","1.4 1.3 1.2 1.1"          # job finish
"2","100000","-1","-1","1.1 1.2"                # job start
"2","100000","130833","1","1.1 1.2"             # a fault stopped the job

---
simulation ending
---

Normally, the simulation will end as soon as the last job finished being
simulated.  This behavior can be adjusted using the schedComponent param:

<useYumYumSimulationKill>true</useYumYumSimulationKill>

If this param is used, then the simulation will not end when the last job is
finished, but the scheduler will block while waiting for the job trace to be
modified.  When the job log is modified, all new jobs are read out of it, and
these are simulated.  This behavior will continue.  The only way to end the
simulation will be to either kill SST, or write "YYKILL" to the job list. This
will cause the schedComponent to unregister itself from SST, and the
simulation will end.

While waiting for the job trace to be written to, schedComponent will poll the
trace file at a frequency defined by

<YumYumPollWait>n</YumYumPollWait>

where n is the number of ms for which schedComponent should sleep between
checking the file for a new modification time.

---
yumyum PRNG
---

The seeds used for the PRNG used for the yumyum functionality discussed here can
be specified as a param to the schedComponent in the XML, like so:

  <params>
    <seed>42</seed>
  </params>

If a seed is not specified, the PRNG will be seeded with the current time, with
a one second resolution.

This seed is used only for yumyum functionality; all other SST functionality
remains the same, as does functionality in other elements.

Various parts of the simulator use different RNGs.  Fault Activation, Error Log
Probability, Error Latency, Error Correction, and Job Kill Probability can all
be seeded seperately.  This can be used to change the RNG of one part of the
simulation while leaving the rest the same.  Whatever seeds are not specified
will be seeded using the main <seed> parameter, or the current time if a <seed>
parameter is not given.  For example:

  <params>
    <seed>42</seed>
    <faultSeed>101</faultSeed>
  </params>

will seed all parts of the simulation with 42, except for the fault activation
which will be seeded with 101.  The seed parameters are:

Fault Activation: <faultSeed></faultSeed>
Error Log Probability: <errorLogSeed></errorLogSeed>
Error Latency Determination: <errorLatencySeed></errorLatencySeed>
Error Correction Probability: <errorCorrectionSeed></errorCorrectionSeed>
Job Kill Probability: <jobKillSeed></jobKillSeed>

Any combination of seed parameters may be used with each other.

---
fault injection
---

Fault activation events may be injected into a running simulation via the use
of a faultInjectionComponent.  This is a special component which periodically
examines a CSV file for pairs of node IDs and fault types, and injects the
specified fault activation event into the given node.  The node treats the new
fault like any other generated fault after it is injected.

Sample faultInjectionComponent:

  <component name="faultInjectionComponent" type="scheduler.faultInjectionComponent">
    <params>
      <faultInjectionFilename>failureEvents.csv</faultInjectionFilename>
      <injectionFrequency>2000000</injectionFrequency>
    </params>
    <link name="fault-1.1" latency="0 ns" port="nodeLink0" />
    <link name="fault-1.2" latency="0 ns" port="nodeLink1" />
    <link name="fault-1.3" latency="0 ns" port="nodeLink2" />
    <link name="fault-1.4" latency="0 ns" port="nodeLink3" />
    <link name="fault-2.1" latency="0 ns" port="nodeLink4" />
    <link name="fault-2.2" latency="0 ns" port="nodeLink5" />
    <link name="fault-4.1" latency="0 ns" port="nodeLink6" />
  </component>

Sample link from a nodeComponent:

<link name="fault-1.1" latency="0 ns" port="faultInjector" />

Sample failure CSV:

4.1, testFailType
2.2, testCoolFailType
1.3, testAwesomeFailType
YYRESUME

The following parameters are available for this component:

REQUIRED: Path and filename containing the node and fialure type pairs:
    <faultInjectionFilename>[filename]</faultInjectionFilename>
OPTIONAL: Token which appears in the file to resume the simulation after all
pairs have been read: <resumeSimToken>[token]</resumeSimToken>  Default value
is YYRESUME
REQUIRED: number of simulatedseconds which should pass between readings of the
failure file: <injectionFrequency>[simulated seconds]</injectionFrequency>
OPTIONAL: number of real wallclock milliseconds which should pass between
examinations of the failure file: <filePollFreq>[ms]</filePollFreq>  Default
value is 1000

After a simulation begins, every injectionFrequency simulated seconds,
failtInjectionFilename is cleared of its contents, signifying that the
simulator is now ready for input, then faultInjectionFilename is read, by
examining the file every filePollFreq milliseconds to determine if the file's
write time has changed.  Once the time has changed, the file's contents are
examined to determine if resumeSimToken appears in it.  If the token does not
appear, the file examination process will be restarted.

Once the token is found, all lines before the token are parsed as node ID and
fault type pairs, and fault activation events of the specified type will be
injected into the specified nodes, and the faultInjectionComponent will wait
injectionFrequency simulated seconds before restarting this process again.

Injected faultActivationEvents are treated like any other fault events once
given to the nodeComponents.  nodeComponents do no need to have any knowledge
of the specified fault type.  Without any configuration concerning the fault
type, default values will be used for error correction, logging, etc.  These
configuration options may be specified if desired.

An example of this component in action can be found in:
  yumyum/examples/4.fileDrivenFaults.xml
  yumyum/examples/testFail.csv

---
yumyum functionality tests
---

A set of tests have been created which can be used to verify that the yumyum
functionality is working correctly.  These tests can be found in the yumyum
subdirectory, and can be run (after building SST and Scheduler, of course)
using the runtests.pl script:

perl runtests.pl

This will run a series of simulations that will test nodeComponent and
schedComponent, including yumyum functionality.  A few short simulations are
run to test basic functionality, and many longer tests are run to test that the
simulated results are statistically believable, as well as to test for
regressions in code correctness and performance.

---
yumyum functionality example
---

An example of a small graph with a number of faults and errors can be found
under yumyum/xml/example.xml.

